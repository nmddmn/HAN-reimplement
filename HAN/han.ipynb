{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Attention(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        self.Encoder = nn.GRU(input_size, hidden_size, bidirectional = True)\n        self.MLP = nn.Sequential(nn.Linear(2 * hidden_size, 2 * hidden_size), nn.Tanh())\n        self.context_vector = nn.Parameter(torch.randn(2 * hidden_size))\n        self.softmax = nn.Softmax()\n        \n    def forward(self,x):\n        h,_ = self.Encoder(x)\n        u = self.MLP(h)\n        alpha = self.softmax(u @ self.context_vector)\n        output = torch.sum(alpha * h)\n        \n        return output\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HAN(nn.Module):\n    def __init__(self, word_embed_size, input_size, word_hidden_size, sentence_hidden_size, num_classes):\n        self.We = nn.Parameter(torch.randn([word_embed_size, input_size]))\n        self.WordAttention = Attention(input_size, word_hidden_size)\n        self.SentenceAttention = Attention(2 * word_hidden_size, sentence_hidden_size)\n        self.classifier = nn.Sequential(nn.Linear(2 * sentence_hidden_size, num_classes), nn.Softmax())\n        \n    def forward(self, article):\n        article = article @ self.We\n        s = torch.stack([self.WordAttention(sentence) for sentence in article])\n        v = self.SentenceAttention(s)\n        p = self.classifier(v)\n        \n        return p ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainloader = DataLoader(trainset, batch_size= BATCH_SIZE, \n                         shuffle= False, pin_memory= True)\nvalloader = DataLoader(valset, batch_size= BATCH_SIZE, \n                         shuffle= False, pin_memory= True)\ntestloader = DataLoader(testset, batch_size= BATCH_SIZE, \n                         shuffle= False, pin_memory= True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(data_loader, model, loss_function):\n    model.eval()\n    Loss = 0\n    correct = 0\n    for input, target in enumerate(data_loader):\n        output = model(input)\n        Loss += loss_function(output, label)\n        correct += (output.argmax(0) == y).type(torch.float).sum().item()\n        \n    Loss /= len(data_loader.dataset)\n    accuracy = correct / len(data_loader.dataset)\n    \n    return Loss, accuracy\n\n\ndef train(data_loader, model, optimizer, scheduler, loss_function):\n    model.train()\n    Loss = []\n    for i, (input, label) in enumerate(data_loader)\n        output = model(input)\n        loss = loss_function(output, label)\n        loss.backward()\n        optimizer.zero_grad()\n        optimizer.step()\n        scheduler.step()\n\n        Loss.append(loss.cpu().detach().numpy())\n        print(\"loss {}\".format(Loss[-1]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = HAN(word_embed_size, input_size, word_hidden_size, sentence_hidden_size, num_classes).to(DEVICE)\n\nlr = 1e-3\nepochs = 3\niteration = epochs * len(trainloader)\noptimizer = optim.Adam(model.parameters(), lr = lr)\nloss_function = torch.nn.NLLLoss()\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = iteration)\n\niter_loss = []\nepoch_loss = []\nbest_acc = 0\nfor t in range(epochs):\n    print(f'Epoch {t} starts.')\n    tloss = train(trainloader, model, optimizer, lr_scheduler, loss_function)\n    val_loss, val_acc = test(valloader, model, LOSS_FN)\n    \n    iter_loss = iter_loss + tloss\n    epoch_loss.append(sum(tloss) / len(tloss))\n    \n    print(f'Epoch {t}: LOSS = {epoch_loss[-1]}, VAL-ACC = {val_acc}')\n    \ntorch.save(model.state_dict(), f'HAN_last.pth')\n    \nprint(model)","metadata":{},"execution_count":null,"outputs":[]}]}